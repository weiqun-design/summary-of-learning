An overview of gradient descent optimization algorithms∗
参考https://www.cnblogs.com/guoyaohua/p/8542554.html
参考https://arxiv.org/pdf/1609.04747.pdf

1. batch gradient descent
特点：对训练集整体计算梯度，再更新parameter，计算较慢，不能进行线上动态更新训练模型，
最终能求出凸函数的全局最优解，或凹函数的局部最优解

2. Stochastic gradient descent
特点：对比bgd计算更快，可以线上训练，sgd计算过程会出现波动
由于波动的存在可能会跳出局部，获得更优的局部最优解，
学习率较小时，bgd的结果与sgd结果相同
每次都将训练数据顺序打乱，每次只计算某条训练数据的梯度，更新参数

3. Mini-batch gradient descent
取两者所长
每次计算n条训练数据的梯度，更新对应的参数，n通常取50-256


Mini-batch gradient descent 小批量梯度下降存在的一些问题：
1. 学习率太小时，会引起收敛速度慢，甚至会导致损失函数在最优解附近波动甚至发散
2. 学习率及相关学习计划必须预先设定，因此适应性较差
3. 相同的学习率应用于所有的feature，而实际上我们可能希望对部分feature进行较大的更新
4. 存在某些SGD很难逃逸的点，所有维度的梯度都接近于零。从而仅得到非凸函数的局部最优解


克服以上问题的方法：

1. 动量方法
对于某一维度的斜率过于陡峭，导致SGD震荡。 通过动量方法可以在其它方向加速
vt = γvt−1 + η∇θJ(θ)
θ = θ − vt
通常lambda 设置为0.9

2. 牛顿加速方法
vt = γ vt−1 + η∇θJ(θ − γvt−1)
θ = θ − vt

3. Adagrad
这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性。
缺点是容易造成学习率不断下降，最终迭代效果消失

4. Adadelta

该方法可以将adagrad不断变小的学习率最终稳定在某个固定值
通常lambda 设置为0.9

gt = ∇θ(J)
θ = θ + (-η/RMS(gt)**-0.5 * gt)


5. RMSprop
与Adadelta第一种形式相同，Hinton 建议设定 γ 为 0.9, 学习率 η 为 0.001

6. Adam
这个算法是另一种计算每个参数的自适应学习率的方法。相当于 RMSprop + Momentum
除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：
建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8


Adagrad, Adadelta, RMSprop, Adam 在这种情景下会更合适而且收敛性更好


三、 如何选择优化算法

如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。
RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。
Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，
随着梯度变的稀疏，Adam 比 RMSprop 效果会好。
整体来讲，Adam 是最好的选择。
很多论文里都会用 SGD，没有 momentum 等。SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点。
如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。




