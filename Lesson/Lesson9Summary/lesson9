1. KNN查找离目标数据最近的前k个数据项
2. ANN近似最近邻检索，在牺牲可接受返回内的精度的情况下提高检索效率
3. 最近邻检索是线性复杂度的，当处理大规模数据时可以采用ANN方法
4. LSH, 局部敏感哈希是ANN的一种
5. 主要的索引技术：基于树的索引技术（二叉树、B-Tree、B+Tree）
                 基于哈希的索引技术
                 基于词的倒排索引
6. 如果数据是低维而且小数据，通过线性的方式查找；数据不仅海量，而且高维，需要降维，采用索引方式查找
7. 传统的HashTable用于检索数据，无法将相似的数据放到同一个bucket（桶）中。
8. LSH将相邻的数据，通过映射后依然保持相邻的关系，即保持局部的敏感度Locality-Sensitive
9. LSH: 通过Hash Function，每个Bucket会落入一些原始数据，属于同一个桶内的数据有很大可能是相邻的（也存在不相邻的数据被hash到了同一个桶内的情况）
        将原始数据集合分成了多个子集合，每个子集合中的数据大概率是相邻的，而且子集合中元素个数较少
        方便进行近邻查找=> 在一个很小的集合里查找相邻元素
10. Hash一般翻译为散列，把任意长度的输入，通过散列算法，变换称固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，
    散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。
    对于不同的关键字可能得到同一散列地址，这种现象称碰撞。
11. 根据散列函数和处理冲突的方法将一组关键字映象到一个有限的连续的地址集，并以关键字在地址集上的"象"作为记录在表中的存储位置，这种表称为
    散列表，这一映像过程称为散列造表或散列，所得的存储位置称散列地址。
12. 所有散列函数都有如下一个基本特性：如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。另一方面，
    散列函数的输入和输入不是一一对应的，如果两个散列值相同，两个输入值很可能是相同的，但并不能绝对肯定二者一定相同。
13. 输入一些数据计算出散列值，然后部分改变输入值，一个具有强混淆特性的散列函数会产生一个完全不同的散列值。
14. 在某些情况下，散列函数可以设计成具有相同大小的定义域和值域间的一一对应。一一对应的散列函数也称为排列。
15. 可逆性可以通过使用一系列的对于输入值的可逆混合运算而得到
16. 常用的hash函数：直接取余法、乘法取整法、平方取中法
17. 著名的hash算法MD5和sha1，它们都是以md4为基础设计的。
18. MD4是麻省理工教授Ronald Rivest于1990年设计的一种信息摘要算法
19. MD5算法具有以下特点：1. 压缩性：任意长度的数据，算出的MD5值长度都是固定的。
                      2. 容易计算：从愿数据计算出MD5值很容易
                      3. 抗修改性：对原数据进行任何改动，哪怕只修改一个字节，所得到的MD5值都有很大区别
                      4. 强抗碰撞：已知原数据和其MD5值，想找到一个具有相同MD5的数据（即伪造数据）是非常困难的
20. SHA1对长度小于264的输入，产生长度为160bit的散列值，因此抗穷举性更好。
21. 散列函数应用：错误校正：使用一个散列函数可以很直观的检测出数据在传输时发生的错误。在数据的发送方，对将要发送的数据应用散列函数，并将计算的结果
    同原始数据一同发送。在数据的接收方，同样的散列函数被再一次应用到接收到的数据上，如果两次散列哈数计算出来的结果不一致，那么就说明数据在传输过程
    中某些地方有错误了。这就叫做冗余校验。
22.
23.
24.
25.
26.
27.
28.
29.
30. YouTube推荐系统，推荐系统分为召回（候选集生成）和排序两个阶段
31. 召回阶段，基于用户画像及场景数据从海量的视频库中将相关度最高的资源检索出来，作为候选集
32. 召回阶段可以通过粗糙的方式召回候选item
33. 排序阶段，基于更加精细的特征对候选集（百级别）进行排序，最终呈现给用户的是很少一部分数据。
34. Ranking阶段，采用更精细的特征计算user-item之间的排序score，作为最终输出推荐结果的依据。
35. 召回阶段的DNN模型：把推荐系统堪称一个超大规模多分类问题，用户信息、上下文信息作为输入条件。
36. 召回阶段DNN模型输入是用户浏览历史、搜索历史、人口统计学信息和其余上下文信息concat生成的输入向量。
37. 模型架构中间是三个隐层的DNN结构，输出分线上和离线训练两个部分。
38. 正负样本和上下文选择：1. 使用更广泛的数据源，训练样本要用youtube上的所有视频观看记录，而不只是系统推荐的视频观看记录。否则，面对新视频的
                       时候很难推荐，并且推荐器会过度偏向exploitation
                       2. 为每个用户生产固定数量的训练样本，在损失函数中所有用户的权重一样，防止一部分非常活跃的用户主导损失函数值
39. 样本和上下文选择中的不对称的统统浏览问题：1. 用户在浏览视频时候，往往都是序列式的，通常会先看一些比较流行的，然后才是观看一些小众的视频。剧集
                                        系列通常也是顺序地观看
                                        2. 线上A/B test方式显示利用上文信息预估下一次浏览视频的predicting next watch方式要好于利用上下文
                                        信息预估中间的视频的held-out效果
                                        3. 方法：从用户的历史视频观看记录中随机拿出来一个作为正样本，然后只用这个视频之前的历史观看记录作为输入
40. 采用负样本，随机从全量item中抽取用户没有点击过的item作为label=0的item后，效果明显提升。
41. 在当次展现的情况下，虽然用户只点击了click的item，其他item没有点击，但是很多用户在后续浏览的时候未click的item也在其他非列表页的地方进行click，
    如果该item标记为label=0，可能是误标记。
42. 推荐列表展示的item极有可能为热门item，虽然该item该用户未点击，但是我们不能降低item的权重
43. 随着网络深度加大，预测准确率在提升，但增加第4层之后，MAP（Mean Average Precision) 已经变化不大了
44. 增加了观看历史以外的特征，对预测准确率提升很明显
45. 排序阶段的建模（对观看时间）
    CTR指标对视频搜索具有一定的欺骗性，所以论文提出采用期望观看时间作为评估指标
    观看时长不是只有0，1两种标签，所以youtube采用了weighted logistic regression来模拟这个输出
46. 排序阶段的特征工程（Feature Engineering）
47. 尽管DNN能够减轻人工特征工程的负担，但是依然需要花费精力将用户及视频数据转化为有效的特征；难点在于对用户行为序列建模，并关联视频打分机制；
    用户对于某channel的历史行为很重要，比如浏览该频道的次数，最近一次浏览该频道距离现在的时间。
48. 把召回阶段的信息传播到ranking阶段同样能提升效果，比如推荐来源和所在来源的分数
49. youtube对于相同域的特征可以共享embedding，好处在于加速迭代，降低内存开销。
50. 实验证明，youtube采用tower塔式模型效果最好，第一层1024，第二层512，第三层256
51.
