1. 强化学习（Reinforcement Learning）：机器学习的一个分支：监督学习、无监督学习、强化学习
                                     强化学习的思路和人比较类似，是在实践中学习
                                     与监督学习的区别，没有监督学习已经准备好的训练数据输出值，强化学习只有奖励值；
2. 强化学习可以应用于不同领域：神经科学、心理学、计算机科学、工程领域、数学、经济学等
3. 强化学习的特点：没有监督数据、只有奖励信号
                 奖励信号不一定是实时的，很可能是延后的，甚至延后很多
                 时间（序列）是一个重要因素
                 当前的行为影响后续接收到的数据
                 强化学习有广泛的应用：游戏AI，推荐系统，机器人仿真，投资管理，发电站控制
4. 强化学习与机器学习：强化学习没有教师信号，也没有label，即没有直接指令告诉机器该执行什么动作
                    反馈有延时，不能立即返回
                    输入数据是序列数据，是一个连续的决策过程
5. 强化学习基本概念：个体 agent 学习器的角色，也称为智能体
                  环境 environment agent之外一切组成的、与之交互的事物
                  动作 action agent的行为
                  状态 state agent从环境获取的信息
                  奖励 reward 环境对于动作的反馈
                  策略 policy agent根据状态进行下一步动作的函数
                  状态转移概率，agent作出动作后进入下一状态的概率
                  四个重要的要素：状态（state）、动作（action）、策略（policy）、奖励（reward）
6. 强化学习：RL考虑的是个体与环境的交互问题
            目标是找到一个最优策略，使agent获得尽可能多的来自环境的奖励
            很多情况下，agent无法获取全部的环境信息，而是通过观察来表示环境，也就是得到自身周围的信息
7. 强化学习agent：
            基于价值的强化学习，value-based，通过学习价值函数指导策略指定
            基于策略的强化学习，policy-based，没有价值函数，直接学习策略
            结果策略梯度以及价值函数的强化学习，actor-critic
            既学习价值函数也学习策略的方法
8. 策略网络就是，对于给定的输入，通过学习给出一个确定输出的网络
9. 通过计算目前状态的累积分数的期望，价值网络给游戏中的状态赋予一个分数，每个状态都经历了整个数值网络
10. 策略网络的输出，是一个落子的概率；价值网络的输出，一个可能获胜的数值，即价值
11. QLearning:Q函数，也称为动作值函数，有两个输入：[状态]和【动作】，它将返回在这个状态下执行该动作的未来奖励期望
12. 随机策略有时候是最优的：对于石头剪刀布的游戏，只要一方有一个确定性的策略，就会被对手抓住进而从整体上输掉。最好的策略是随机选择每次出法，
    以得到最大可能的总体奖励。
13. policy-based优势：收敛性好，因为policy-based每次只改善一点点，但总是朝着好的方向改善，而有些价值函数在后期会一直围绕最优价值函数持
                     续小的震荡而不收敛。
                     对于高维度或连续状态空间来说，更高效，使用基于价值函数的学习在得到价值函数后，制定策略时，需要比较各种行为对应的价值
                     大小，这个比较过程就比较难，而采用policy-based会高效很多。
                     能学到一些随机策略，而value-based通常是学不到随机策略
                     针对价值函数非常复杂的情况，采用policy-based更适合。
14. 强化学习问题：首先把要解决的问题转化为一个环境（environment）
    状态空间：对于围棋来说，每一个棋盘布局就是一个状态，所有可能的棋盘布局就是状态空间
    动作空间：对于围棋来说，所有可能落子的位置就是一个动作空间
    可行动作：给定一个棋盘，哪里可以落子，哪里不可以
    状态转化：下棋以后，对手可能会下的棋，如果是两个alpha zero对弈的情况，相互是对方环境的一个部分
    奖励函数：下棋之后得到的信号反馈。在围棋里面，就是胜率的一个正函数。胜率越大，奖励越大。
15. alphaGo zero策略：在alphaGo系列算法里面是使用蒙特卡洛树搜索（MCTS）来进行策略优化的
16. 蒙特卡洛算法：
    并不是一种算法的名称，而是对一类随机算法的特性的概括
    原理：采用越多，越接近最优解
    如果要求解的问题，可以在有限采样内，必须给出一个解，这个解不要求一定是最优解，那么就可以采用蒙特卡洛算法
17. MCTS原理：每个节点代表一个局面，A/B代表被访问B次，黑棋赢了A次
             step1：选择select，从根节点往下走，每次都选一个最有价值的子节点，直到找到存在未扩展的子节点，即这个局面存在未走过的
             后续走法的节点。
             step2：扩展Expansion，给这个节点加上一个0/0子节点，对应之前所说的未扩展的子节点
             step3：模拟simluation，用快速走子策略（rollout policy）走到底，得到一个胜负结果
             step4：回传backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是0/1，就把0/1加到所有父节点上
18. 在select过程中，如何判断节点的价值：
    如果采用贪心算法策略，即每次都只选择最有利的，如果搜索树广度不够，容易忽略实际更好的选择
    简单有效的选择公式
19. UCB算法（upper confidence bound）：
    用于select过程中，判断节点的价值，每次选择选取最大的ucb节点：
    UCB = Q(s,a) + U(s,a)
20. alphaGo的策略网络，让我们更准确地选择需要扩展的节点
    alphaGo的价值网络，可以与快速走子策略的模拟结果相结合，得到更准确的局面评估结果
    策略网络可以让我们对动作使用输出概率来表示，对于离散动作空间，使用softmax动作的出现概率；对于连续空间使用高斯分布来获取动作的概率
21. 马尔可夫决策过程
    马尔可夫决策过程为决策者在随机环境下做出决策提供了数学架构模型，为动态规划与强化学习的最优化问题提供了有效的数学工具，
    广泛用于机器人学，自动化控制，经济学，以及工业界等领域。一般特指其在离散时间中的随机控制过程：即对于每个时间节点，当该过程处于某状态(s)
    时，决策者可采取在该状态下被允许的任意决策(a),此后下一步系统状态将随机产生，同时回馈给决策者相应的期望值 Ra(S,S'),该状态具有马尔可夫性质。
22. 强化学习是机器学习大家族中的一大类，使用强化学习能够让给机器学着如何在环境中拿到高分，表现出优秀的成绩，而这些成绩背后却是他所付出的辛苦劳动，
    不断的试错，不断地尝试，累积经验，学习经验。
23. 强化学习是一个大家族，他包含了很多中算法
24. 强化学习方法汇总：了解强化学习中常用到的几种方法，以及他们的区别，对我们根据特定问题选择方法时很有帮助。强化学习是一个大家族，发展历史也不短，具有很多种不同方法，
    比如：Q learning， Policy Gradients，还有对环境的理解的model-based RL等等。
25. model-free：不理解环境；model-based RL 机器人：理解环境
26. model-based多出了一个虚拟环境，不仅能在现实中玩耍，还能在游戏中玩耍，model-based有想象力。
27. model-free中，机器人只能按部就班，一步一步等待真实世界的反馈，再根据反馈采取下一步行动。而model-based，他能通过想象来预判断接下来将要发生的所有情况。
    然后选择这些想象情况中最好的那种。
28. model-free的方法：Q learning， Sarsa, Policy Gradients
29. 基于概率和基于价值，基于概率是强化学习中最直接的一种，他能通过感官分析所处的环境，直接输出下一步要采取的各种动作的概率，然后根据概率采取行动，所以每种动作都有可能被
    选中，只是可能性不同。而基于价值的方法输出则是所有动作的价值，选择最高值来行动。
30. 选取连续动作时，基于价值的方法是无能为力的。基于概率的方法可以解决
31. 基于概率，有policy gradients
32. 基于价值，有Q learning， Sarsa等
33. 结合这两类方法的优势之处，可以创造更牛逼的一种方法，叫做actor-critic,actor就基于概率作出动作，而critic会对作出的动作给出动作的价值
34. 回合更新和单步更新。回合更新指游戏开始后，要等待游戏结束，然后再总结这一回合中的所有转折点。再更新我们的行为准则。
    单步更新则是在游戏进行中每一步都在更新，不用等待游戏的结束。
35. monte-carlo learning和基础版policy grandients等都是回合更新制；
    Qlearning，sarsa，升级版的policy gradients都是单步更新制。
36. 因为单步更新制更有效率，所以现在大多方法都是基于单步更新。
37. 在线学习和离线学习
38. 在线学习一定是本人边玩边学习，而离线学习是你可以选择自己玩，也可以选择看着别人玩，通过看别人玩来学习别人的行为准则。
39. 




















