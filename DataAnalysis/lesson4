决策树模型
1. 衡量节点数据集合的有序性（纯度）如下：熵，基尼，方差。其中熵和基尼针对分类，方差针对回归。
2. ID3算法采用信息增益作为衡量标准，选择信息增益最大的属性作为分裂属性。ID3算法存在的问题为倾向于多值属性。
3. C4.5采用信息增益率的方式来作为选择属性。信息增益率 = 信息增益/属性熵，C4.5采用悲观剪枝，可以提高决策树的泛化能力
4. PEP是后剪枝技术的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。
5. ID3和C4.5算法可以生成二叉树或多叉树，而CART只支持二叉树。
6. CART分类树与C4.5类似，只是属性选择的指标采用的是基尼系数。
7. 经济学中基尼系数大于0.4的时候，财富差异悬殊。基尼系数在0.2-0.4之间说明分配合理，财富差距不大。
8. 基尼系数本身反映了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。CART在构造分类树的时候，
   采用了基尼系数最小的属性作为属性的划分
9. 节点D的基尼系数等于子节点D1和D2的归一化基尼系数之和
10. 可视化决策树 dot out.dot -T pdf -o out.pdf
11. cart决策树的剪枝主要采用CCP方法，它是一种后剪枝的方法，英文全称叫做cost-complexity prune.


贝叶斯分类
1. 先验概率，通过经验来判断事情发生的概率
2. 后验概率，发生结果之后，推测原因的概率
3. 条件概率，发生结果之后，推测原因的概率
4. 朴素贝叶斯算法，假设每个输入变量是独立的。但是实际情况并不一定。
5. 贝叶斯原理是最大的概念，它解决了概率论中逆向概率的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器的一种
6. 朴素贝叶斯常用于文本分类，尤其对于英文等语言来说，分类效果很好。如垃圾文本过滤，情感预测，推荐系统等。常用于自然语言处理NLP的工具。
7. sklearn中提供了三种朴素贝叶斯分类算法，分别是高斯朴素贝叶斯，多项式朴素贝叶斯，伯努利朴素贝叶斯。
8. 高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布即正态分布
9. 多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，用于处理一次实验有多个可能的结果的情况
10. 伯努利朴素贝叶斯：特征变量是布尔变量，符合0/1分布
11. TF-IDF 词频TF计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数成正比，逆向文档频率IDF，是指一个单词在文档
    中的区分度，它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其它文档区分开
12. TF = 单词出现的次数/该文档的总单词数
13. 逆向文档频率IDF = log（文档总数/该单词出现的文档数+1）  +1用于避免存在分母为0的情况
14. 文档分类过程两个重要阶段：基于分词的数据准备，用于朴素贝叶斯分类进行分类
15. 基于分词的数据准备，包括分词、单词权重计算、去掉停用词


SVM  公式推导待学习
1. SVM是有监督的学习模型，通常用于模式识别、分类以及回归分析
2. SVM的计算过程是寻找超平面的过程
3. 极限位置A和B之间的分类线C就是最优决策面，极限位置到最优决策面C之间的距离，就是分类间隔，margin
4. 完全线性可分情况下的线性分类器，也就是线性可分的情况下，是最原始的SVM，它的核心思想就是找到最大的分类间隔。
5. 大部分线性可分情况下的线性分类器，引入了软间隔的概念。软间隔就是允许一定量的样本分类错误
6. 线性不可分情况下的非线性分类器，引入了核函数。它让原有的样本空间通过核函数投射到一个高维的空间中，从而变得线性可分
7. SVM本身是一个单分类器，解决多分类问题时，可以使用一对多法和一对一法。
8. 一对多法：A作为正集, B，C，D作为负集
9. 一对一法：任意两个类别之间构造一个分类器
10. from sklearn import svm
    model_1 = svm.LinearSVC()  # 线性分类器，仅只能使用线性核函数
    model_2 = svm.SVC(kernel='rbf', C=1.0, gamma='auto') # kernel为核函数，linear为线性核函数，poly为多项式核函数，rbf为高斯核函数，sigmoid为sigmoid核函数
11. 线性核函数是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。
12. 多项式核函数可以将数据从低维空间映射到高维空间，但参数比较多，计算量大
13. 高斯核函数同样可以将样本映射到高维空间，相比于多项式核函数参数较少，通常性能不错，所以是默认使用的核函数
14. 当选用sigmoid核函数时，SVM实现的是多层神经网络
15. 参数C是目标函数的惩罚系数，惩罚系数指的是分错样本时的惩罚程度，默认情况下为1.0，当C越大时，分类器的准确性越高，但同样容错率会降低，泛化能力会变差。相反，C越小，泛化能力越强，但是准确性会降低。
16. gamma代表核函数的系数，默认为样本特征数的倒数


KNN算法
1. 工作原理如下：计算待分类物体与其他物体之间的距离；统计距离最近的K个邻居；对于K个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类。
2. 距离计算：欧氏距离，曼哈顿距离，闵可夫斯基距离，切比雪夫距离，余弦距离
3. 欧氏距离即欧几里得距离
4. 曼哈顿距离为两个点在坐标系上绝对轴距总和。
5. 闵可夫斯基距离是一组距离的定义。一维距离为曼哈顿距离，二维距离为欧氏距离，无限维距离为切比雪夫距离
6. 切比雪夫距离，两个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值
7. 余弦距离是两个向量的夹角，在兴趣相关性分析上，角度关系比距离的绝对值更重要
8. KD树是对数据点在K维空间中划分的一种数据结构，在KD树的构造中，每个节点都是K维数值点的二叉树
9. KNN的理论简单直接，但是当数据量大的时候，计算量是非常庞大的，需要大量的存储空间和计算时间,另外如果样本分类不均衡，比如有些分类的样本非常少，那么该类别的分类准确率就会低很多。
10. 分类调用from sklearn.neighbors import KNeighborsClassifier
11. 回归调用from sklearn.neighbors import KNeighborsRegressor
12. 构造函数 KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)，这里有几个比较主要的参数：
    1. n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。
    2. weights：是用来确定邻居的权重，有三种方式：weights=uniform，代表所有邻居的权重相同；weights=distance，代表权重是距离的倒数，即与距离成反比；自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。
    3. algorithm：用来规定计算邻居的方法，它有四种方式：algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，
    不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；algorithm=brute，
    也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。
    4. leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。


K-Means
1. 工作原理
    1. 选取K个点作为初始的类中心点，这些点一般都是从数据集中随机抽取的
    2. 将每个点分配到最近的类中心点，这样就形成了K类，然后重新计算每个类的中心点
    3. 重复第二步，直到类不发生变化，或者设置最大迭代次数


EM聚类
1. 由于K-means是通过距离来区分样本之间的区别的，且每个样本在计算的时候只能属于一个分类，称之为硬聚类算法。
   而EM聚类在求解过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。
2. 常用的EM聚类有GMM高斯混合模型和HMM隐马尔可夫模型。
3. HMM用到了马尔可夫过程，HMM在自然语言处理和语音识别领域有广泛的应用。
4. GMM是通过概率密度来进行聚类，聚成的类符合高斯分布
5. EM算法相当于一个框架，E步骤相当于通过初始化的参数来估计隐含变量，M步骤就是通过隐含变量反推来优化参数，最后通过EM迭代得到模型参数。


关联规则挖掘
1. 支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。
2. 置信度指的是当你购买了商品A，会有多大的概率购买商品B。
3. 商品推荐中，重点考虑的是提升度，因为提升度代表的是商品A的出现，对商品B的出现概率提升的程度。提升度（A——>B）= 置信度（A——>B) / 支持度（B）
4. 频繁项集就是支持度大于最小支持度阈值的项集。
5. 筛选K=1的频繁项集，在此基础上筛选K=2的频繁项集，我们在筛选掉小于最小值支持度的商品组合，再将商品进行K=3的商品组合，再筛选掉小于最小支持度的商品组合。
6. apriori算法缺点：可能产生大量的候选集，把可能的项集都组合出来了；每次计算都需要重新扫描数据集，来计算每个项集的支持度
7. FP-Growth算法是apriori算法的改进算法，创建来一颗FP树来存储频繁项集，在创建前对不满足最小支持度的项进行删除，减少了储存空间，整个生成过程只
   遍历数据集2次，大大减少了计算量
8. 对apriori算法的改进除了FP-Growth以外，还有CBA算法、GSP算法


PageRank
1. 出链：链接出去的链接
2. 入链：链接进来的链接
3. 网页的影响力 = 所有入链集合的页面的加权影响力之和。
4. pagerank可能遇到的问题：等级泄漏、等级沉默。
5. 等级泄漏：如果某一个网页没有出链，最终会导致其他网页的PR值为0
6. 等级沉默：如果一个网页只有出链，没有入链，计算的过程迭代下来，会导致这个网页的PR值为0。
7. 随机浏览模型，新增阻尼因子d，通常取0.85,PR(u) = 1-d/N + d * (PR(v0)/L(v0) + ....)
8. import networkx as nx
9. G = nx.Graph()  # 创建无向图  G = nx.DiGraph()  # 创建有向图
10. G.add_node("A")  # 添加节点 G.add_nodes_from(["B","C","D","E"]) # 添加节点集合
11. G.remove_node("A") # 删除节点  G.remove_nodes_from(["B","C","D","E"]) # 删除集合中的节点
12. G.nodes() 返回所有的节点
13. G.add_edge("A","B")  # 增加边
14. G.add_edges_from
15. G.add_weighted_edges_from(u,v,w) # 新增有权边，其中w为权重


AdaBoost

1. AdaBoost算法与随机森林算法都属于分类算法中的集成算法。
2. 集成算法通常有两种形式，投票选举（bagging）和再学习（boosting）
3. 投票选举类似于把专家召集到一个会议桌前，让k个专家分别进行分类，然后选择出现次数最多的哪个类作为最终的分类结果
4. 再学习相当于把k个专家进行加权融合，形成一个新的超级专家。
5. adaboost的英文全称是adaptive boosting, 中文含义为自适应提升算法
6. adaboost算法是通过改变样本的数据分布来实现的，adaboost会判断每次训练的样本是否正确分类，对于正确分类的样本，降低它的权重，
   对于被错误分类的样本，增加它的权重




































