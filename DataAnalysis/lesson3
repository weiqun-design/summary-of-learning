1. 商业智能 business intelligence 基于数据仓库，经过数据挖掘，得到商业价值的过程。
2. 数据仓库 data warehouse DW
3. 数据挖掘 data mining 数据挖掘的核心包括分类、聚类、预测、关联分析等任务
4. 元数据 metaData 描述其它数据的数据，也称中介数据
5. 数据元 DataElement 最小数据单元
6. 数据预处理步骤：数据清洗，数据集成，以及数据变换
7. 数据清洗：主要是去除重复数据，去噪声（干扰数据）以及填充缺失值。
8. 数据集成：是将多个数据源中的数据存放于一个统一的数据存储中。
9. 数据变换：通过归一化将属性数据按照比例缩放，这样就可以将数据落入一个特定的区间内，比如0-1之间
10. 数据后处理，将模型预测的结果进一步处理后，再导出。比如二分类问题中，一般能得到的是0-1之间的概率值，
    此时将数据以0.5为界限进行四舍五入就可以实现后处理
11. 数据挖掘的一个英文解释叫 Knowledge Discovery in Database
12. 精细化运营将是长久的主题。

13. 用户唯一标识是整个用户画像的核心,设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人收集号、邮箱、设备号、cookieID等
14. 给用户打标签
    用户标签：它包括了性别、年龄、地域、收入、学历、职业等。
    这些包括了用户的基础属性。消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。
    行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。
    内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。
15. 我们可以从用户生命周期的三个阶段来划分业务价值，包括：获客、粘客和留客。
    获客：如何进行拉新，通过更精准的营销获取客户。
    粘客：个性化推荐，搜索排序，场景运营等。
    留客：流失率预测，分析关键节点降低流失率。

16. 需要多源的数据采集，收集到尽可能多的数据维度，同时保证数据的质量。
17. 数据源： 开放数据源 -- 政府、企业、高校
            爬虫抓取 -- 网页、app
            日志采集 -- 前端采集、后端脚本
            传感器 -- 图像、测速、热敏
18. 三种常用的抓取工具
    1. 火车采集器
    2. 八爪鱼  云采集，自动切换ip
    3. 集搜客
19. 埋点就是在有需要的位置采集相应的信息，进行上报

20. 好的数据分析师必定是一名数据清洗高手，在整个数据分析过程中，无论是在时间上还是在功夫上，数据清洗大概都占到了80%
21. 数据清洗原则
    1. 完整性：单条数据是否存在空值，统计字段是否完善
    2. 全面性：观察某一列的全部数据，查看平均值，最大值，最小值
    3. 合法性：数据的类型、内容、大小的合法性。
    4. 唯一性：数据是否存在重复记录，因为数据通常来自不通渠道的汇总，重复的数据是常见的。
22. 缺失值处理： 删除数据缺失的记录、使用当前列的均值、使用当前列出现频率最高的数据
    df['Age'].fillna(df['Age'].mean(), inplace=True) # 均值替换
    age_maxf = train_features['Age'].value_counts().index[0]
    train_features['Age'].fillna(age_maxf, inplace=True) # 缺失值替换
23. 删除空行
    df.dropna(how='all',inplace=True)
24. # 获取 weight 数据列中单位为 lbs 的数据
    rows_with_lbs = df['weight'].str.contains('lbs').fillna(False)
    print df[rows_with_lbs]
    # 将 lbs转换为 kgs, 2.2lbs=1kgs
    for i,lbs_row in df[rows_with_lbs].iterrows():
    # 截取从头开始到倒数第三个字符之前，即去掉lbs。
        weight = int(float(lbs_row['weight'][:-3])/2.2)
        df.at[i,'weight'] = '{}kgs'.format(weight)
25. 合理性，删除非ascii字符
    df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
    df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
26. 唯一性
    df[['first_name','last_name']] = df['name'].str.split(expand=True)
    df.drop('name', axis=1, inplace=True)
27. 重复数据
    df.drop_duplicates(['first_name','last_name'],inplace=True)
28. 没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。

29. 数据集成
    数据集成是数据工程师要做的工作之一。一般来说，数据工程师的工作包括了数据的ETL和数据挖掘算法的实现。
30. ETL extract、transform、load，包括了数据抽取、转换、加载三个过程。
31. 数据集成可以分为ETL和ELT两种架构
    ETL的过程为抽取——转换——加载，抽取后转换，然后写入目的地
    ELT的过程为提取——加载——变换，在抽取后将结果线写入目的地，然后利用数据库的聚合分析能力或者外部计算框架，如spark来完成转换的目的
32. ELT 和 ETL 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 ELT 方法，在提取完成之后，
    数据加载会立即开始。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支
    持业务。在 ELT 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中
    提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程
33. 典型的ELT工具
    商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等
    开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等
34. 下载地址https://community.hitachivantara.com/docs/DOC-1009855

35. 数据变换是数据准备的重要环节，它通过数据平滑、数据聚集、数据概化和规范化等方式将数据转换成适用于数据挖掘的形式。
36. 数据平滑：去除数据中的噪声，将连续数据离散化。采用分箱、聚类、回归的方式进行数据平滑
37. 数据聚集：对数据进行汇总，在sql中有一些聚集函数可以供我们操作，比如max(),sum()
38. 数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如上海、杭州、深圳、北京可以概化为中国。
39. 数据规范化：使属性数据按比例缩放，将原来的数值映射到一个新的特定区域中，常见的方法有min-max归一化、z-score归一化、小数定标。
40. 属性构造：构造出新的属性并添加到属性集中。